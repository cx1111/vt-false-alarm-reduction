{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1 - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0 - Get signal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import periodogram\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import Imputer\n",
    "import wfdb\n",
    "from wfdb import processing\n",
    "\n",
    "from vt.records import get_alarms, data_dir\n",
    "from vt.features import calc_moments, calc_spectral_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alarms, record_names, record_names_true, record_names_false = get_alarms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_features(record_name):\n",
    "    \"\"\"\n",
    "    Aggregate function. Calculate all features for the last 10s of a record.\n",
    "    \n",
    "    Features for each signal are:\n",
    "    - Moments: mean, std, skew, kurtosis\n",
    "    - Number of beats detected\n",
    "    - Average heart rate\n",
    "    - Spectral band power ratios\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    record_name : str\n",
    "        The record name\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    features : pandas dataframe\n",
    "        Dataframe of the calculated features\n",
    "\n",
    "    \"\"\"\n",
    "    fs = 250\n",
    "    start_sec = 290\n",
    "    stop_sec = 300\n",
    "    \n",
    "    # Desired features\n",
    "    features = []\n",
    "    # Features are calculated for each individual signal\n",
    "    feature_labels = [['_'.join([moment, str(ch)]) for moment in ['mean', 'std', 'skew',\n",
    "                                                                  'kurt', 'n_beats', 'hr',\n",
    "                                                                  'lfp', 'mfp', 'hfp']] for ch in range(3)]\n",
    "    feature_labels = [x for y in feature_labels for x in y] + ['result']\n",
    "    \n",
    "    # Read record\n",
    "    signal, fields = wfdb.rdsamp(os.path.join(data_dir, record_name),\n",
    "                                 sampfrom=start_sec*fs, sampto=stop_sec*fs,\n",
    "                                 channels=[0, 1, 2])\n",
    "    \n",
    "    # Get beat locations\n",
    "    qrs_0 = processing.xqrs_detect(signal[:, 0], fs=fs, verbose=False)\n",
    "    qrs_1 = processing.xqrs_detect(signal[:, 1], fs=fs, verbose=False)\n",
    "    pulse_2 = wfdb.rdann(os.path.join(data_dir, record_name), 'wabp2',\n",
    "                         sampfrom = start_sec*fs, sampto = stop_sec*fs,\n",
    "                         shift_samps=True).sample\n",
    "    \n",
    "    beat_inds = [qrs_0, qrs_1, pulse_2]\n",
    "    \n",
    "    # Calculate features for each signal\n",
    "    for ch in range(3):\n",
    "        # Moments\n",
    "        features = features + list(calc_moments(signal[:,ch]))\n",
    "        \n",
    "        # Beat information\n",
    "        rr = processing.calc_rr(qrs_locs=beat_inds[ch])\n",
    "        n_beats = len(rr)\n",
    "        hr = processing.calc_mean_hr(rr=rr, fs=fs)\n",
    "        features = features + [n_beats, hr]\n",
    "        \n",
    "        # Frequency information\n",
    "        features = features + list(calc_spectral_ratios(signal[:, ch], fs=fs))\n",
    "        \n",
    "    # Add on the alarm label\n",
    "    features = features + [alarms.loc[record_name]['result']]\n",
    "    # Convert to dataframe\n",
    "    features = pd.DataFrame([features], columns=feature_labels, index=[record_name])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate features for all records using multiple cpus\n",
    "pool = Pool(processes=cpu_count() - 1)\n",
    "features = pool.map(calc_features, record_names)\n",
    "\n",
    "# Combine features into a single data frame\n",
    "features = pd.concat(features)\n",
    "\n",
    "# Impute the missing nans\n",
    "imp = Imputer(missing_values='NaN', strategy='mean')\n",
    "imp.fit(features)\n",
    "features = imp.transform(features)\n",
    "\n",
    "print('Finished calculating features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Training and Testing Data\n",
    "\n",
    "- We take a subset of our data as the training set. Supervised classifiers can use this labelled data to learn how to discern between the two outcome categories.\n",
    "- We take the remaining data as the testing set, which we use to evaluate our algorithms/models. This is analagous to new data we have not previously encountered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(features[:, :-1], features[:, -1],\n",
    "                                                    train_size=0.75, test_size=0.25,\n",
    "                                                    random_state=0)\n",
    "print('Number of training records: %d' % len(x_train))\n",
    "print('Number of testing records: %d' % len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Supervised Classifiers\n",
    "\n",
    "A supervised classifier learns parameters from labeled training data (the alarm results). After being trained, it can be used to classify new unlabelled data.\n",
    "\n",
    "Examples:\n",
    "- Logistic regression (LR) http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- K nearest neighbors (KNN) http://scikit-learn.org/stable/modules/neighbors.html\n",
    "- Support vector machine (SVM) http://scikit-learn.org/stable/modules/svm.html\n",
    "- Gradient Boosting (GB) http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "\n",
    "We are using hard classifiers, as opposed to fuzzy/soft classifiers. Each decision falls firmly into one category, rather than outputting a probability.\n",
    "\n",
    "On top of these, we can combine them with with an ensemble method. ie. Voting Classifier http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "\n",
    "\n",
    "*We can also use an unsupervised rule-based classifier, leveraging expert knowledge, rather than relying on a model that is limited by its training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, neighbors\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR\n",
    "clf_lr = LogisticRegression()\n",
    "clf_lr.fit(x_train, y_train)\n",
    "y_predict_lr = clf_lr.predict(x_test)\n",
    "\n",
    "# KNN\n",
    "clf_knn = neighbors.KNeighborsClassifier()\n",
    "clf_knn.fit(x_train, y_train)\n",
    "y_predict_knn = clf_knn.predict(x_test)\n",
    "\n",
    "# SVM\n",
    "clf_svm = svm.SVC()\n",
    "clf_svm.fit(x_train, y_train)\n",
    "y_predict_svm = clf_svm.predict(x_test)\n",
    "\n",
    "# And GB\n",
    "clf_gb = GradientBoostingClassifier()\n",
    "clf_gb.fit(x_train, y_train)\n",
    "y_predict_gb = clf_gb.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - Evaluating Performance\n",
    "\n",
    "In order to determine how well our system performs, we need an objective evaluation function.\n",
    "\n",
    "### The confusion matrix\n",
    "![Confusion Matrix](http://www.dataschool.io/content/images/2015/01/confusion_matrix2.png)\n",
    "\n",
    "### The cost matrix\n",
    "\n",
    "The cost matrix is the confusion matrix weighed by the penalty of each decision result. In our challenge, we assign zero cost to correct predictions (as is the usual case), and the cost of *False Negatives* is 5x as great as the cost of *False Positives*.\n",
    "\n",
    "**`Score = ( TP + TN ) / ( TP + TN + FP + 5*FN )`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_results(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics. Input variables are array-likes of true\n",
    "    outcomes and predicted outcomes.\n",
    "    \n",
    "    Returns the confusion matrix, the proportion of correct predictions,\n",
    "    and the final score\n",
    "    \"\"\"\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    cm = pd.DataFrame(cm, columns=['Predict 0', 'Predict 1'], index=['Actual 0', 'Actual 1'])\n",
    "    \n",
    "    # Correct classification proportion\n",
    "    p_correct = (cm.iloc[0,0]+cm.iloc[1,1])/len(y_pred)\n",
    "    \n",
    "    # Score = ( TP + TN ) / ( TP + TN + FP + 5*FN )\n",
    "    score = calc_final_score(cm)\n",
    "    \n",
    "    return cm, p_correct, score\n",
    "\n",
    "\n",
    "def calc_final_score(cm):\n",
    "    \"\"\"\n",
    "    Calculate final score from a confusion matrix. False negatives\n",
    "    are penalized 5x as much as false positives::\n",
    "    \n",
    "        Score = ( TP + TN ) / ( TP + TN + FP + 5*FN )\n",
    "    \n",
    "    \"\"\"\n",
    "    if type(cm) == pd.DataFrame:\n",
    "        score = ((cm.iloc[1, 1] + cm.iloc[0, 0])\n",
    "                  / (cm.iloc[1, 1] + cm.iloc[0, 0] + cm.iloc[0, 1] + 5*cm.iloc[1, 0]))\n",
    "    elif type(cm) == np.ndarray:\n",
    "        score = (cm[0, 0] + cm[0, 1]) / (cm[1, 1]+cm[0, 1]+cm[0, 1] + 5*cm[1, 0])\n",
    "    \n",
    "    return score\n",
    "\n",
    "def print_results(cm, pcorrect, score, classifier_name=''):\n",
    "    \"\"\"\n",
    "    Display the performance results\n",
    "    \n",
    "    \"\"\"\n",
    "    print('Classifier: %s' % classifier_name)\n",
    "    print('Confusion Matrix:')\n",
    "    display(cm)\n",
    "    print('Proportion Correct:', pcorrect)\n",
    "    print('Final Score:', score)\n",
    "    print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm, p_correct, score = calc_results(y_test, y_predict_knn)\n",
    "print_results(cm, p_correct, score, 'KNN')\n",
    "\n",
    "cm, p_correct, score = calc_results(y_test, y_predict_svm)\n",
    "print_results(cm, p_correct, score, 'SVM')\n",
    "\n",
    "cm, p_correct, score = calc_results(y_test, y_predict_lr)\n",
    "print_results(cm, p_correct, score, 'LR')\n",
    "\n",
    "cm, p_correct, score = calc_results(y_test, y_predict_gb)\n",
    "print_results(cm, p_correct, score, 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Most classifications are correct, yet the score is low because of the disproportionate penalty of false negatives. Because there are many more cases of false alarms, the trained models may favor outputting 'False'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%d training records - %d false alarms, %d true alarms'\n",
    "      % (len(y_train), len(np.where(y_train==0)[0]), len(np.where(y_train==1)[0])))\n",
    "\n",
    "print('%d testing records - %d false alarms, %d true alarms'\n",
    "      % (len(y_test), len(np.where(y_test==0)[0]), len(np.where(y_test==1)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
